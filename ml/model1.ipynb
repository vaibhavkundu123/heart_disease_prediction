{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries for data manipulation, model training, and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Add, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Confirm libraries are loaded\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Here are the first few rows:\n",
      "  Patient_ID  Height_cm  Weight_kg Blood_Pressure  Temperature_C  Heart_Rate  \\\n",
      "0      P0001      174.0       59.0         120/80           36.6        63.0   \n",
      "1      P0002        NaN       67.0         130/85           37.0        68.0   \n",
      "2      P0003      176.0       58.0         140/90           37.0        65.0   \n",
      "3      P0004      185.0       65.0         125/82           37.2        69.0   \n",
      "4      P0005      167.0       41.0         135/88           36.3        72.0   \n",
      "\n",
      "              Symptoms Existing_Conditions      Disease_Predictions  \\\n",
      "0           chest pain            Diabetes            Heart Disease   \n",
      "1  shortness of breath        Hypertension  Coronary Artery Disease   \n",
      "2              fatigue    High Cholesterol               Arrhythmia   \n",
      "3            dizziness                 NaN                      NaN   \n",
      "4         palpitations              Asthma             Hypertension   \n",
      "\n",
      "  Laboratory_Test_Results  Cholesterol_mg_dL  Blood_Sugar_mg_dL  \\\n",
      "0        High Cholesterol              198.0               79.0   \n",
      "1                     NaN              179.0               90.0   \n",
      "2                  Normal              193.0               82.0   \n",
      "3                     NaN              228.0              103.0   \n",
      "4                     NaN              251.0              135.0   \n",
      "\n",
      "  Family_History_Heart_Disease Smoking_Status  \n",
      "0                          Yes          Never  \n",
      "1                           No         Former  \n",
      "2                          NaN        Current  \n",
      "3                           No          Never  \n",
      "4                          Yes         Former  \n"
     ]
    }
   ],
   "source": [
    "# Loading dataset from CSV file to analyze and preprocess\n",
    "data_path = 'Patient_Health_Data.csv'\n",
    "patient_data = pd.read_csv(data_path)\n",
    "\n",
    "# Print the first few rows to understand the structure of the data\n",
    "print(\"Dataset loaded. Here are the first few rows:\")\n",
    "print(patient_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Data Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Patient_ID                    2000 non-null   object \n",
      " 1   Height_cm                     1915 non-null   float64\n",
      " 2   Weight_kg                     1914 non-null   float64\n",
      " 3   Blood_Pressure                1640 non-null   object \n",
      " 4   Temperature_C                 1892 non-null   float64\n",
      " 5   Heart_Rate                    1903 non-null   float64\n",
      " 6   Symptoms                      1562 non-null   object \n",
      " 7   Existing_Conditions           1488 non-null   object \n",
      " 8   Disease_Predictions           1488 non-null   object \n",
      " 9   Laboratory_Test_Results       1505 non-null   object \n",
      " 10  Cholesterol_mg_dL             1899 non-null   float64\n",
      " 11  Blood_Sugar_mg_dL             1880 non-null   float64\n",
      " 12  Family_History_Heart_Disease  1891 non-null   object \n",
      " 13  Smoking_Status                1911 non-null   object \n",
      "dtypes: float64(6), object(8)\n",
      "memory usage: 218.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset structure:\")\n",
    "print(patient_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the 'Symptoms' column:\n",
      "['High Cholesterol' nan 'Normal' 'High Blood Sugar' 'Low Iron']\n"
     ]
    }
   ],
   "source": [
    "unique_symptoms = patient_data['Laboratory_Test_Results'].unique()\n",
    "print(\"Unique values in the 'Symptoms' column:\")\n",
    "print(unique_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading data\n",
    "print(\"\\nInitial Data Overview:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Dataset shape:\", patient_data.shape)\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "display(patient_data.head())\n",
    "\n",
    "# After preprocessing\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "missing_values = patient_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(patient_data.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Add distribution plots for numerical features\n",
    "numerical_cols = patient_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(3, 4, i)\n",
    "    sns.histplot(patient_data[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting 'Blood_Pressure' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Blood_Pressure' column into 'Systolic_BP' and 'Diastolic_BP'\n",
    "# This separation helps in individual analysis of each blood pressure component\n",
    "patient_data[['Systolic_BP', 'Diastolic_BP']] = patient_data['Blood_Pressure'].str.split('/', expand=True)\n",
    "\n",
    "# Verify the split by viewing the updated DataFrame\n",
    "print(\"Blood pressure split into 'Systolic_BP' and 'Diastolic_BP':\")\n",
    "print(patient_data[['Systolic_BP', 'Diastolic_BP']].head())\n",
    "print(patient_data[['Systolic_BP', 'Diastolic_BP']].info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Blood Pressure Columns to Numeric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the new blood pressure columns to numeric and handle non-numeric entries by setting them to NaN\n",
    "# This is necessary for further analysis since we canâ€™t process string values in these columns\n",
    "patient_data['Systolic_BP'] = pd.to_numeric(patient_data['Systolic_BP'], errors='coerce')\n",
    "patient_data['Diastolic_BP'] = pd.to_numeric(patient_data['Diastolic_BP'], errors='coerce')\n",
    "\n",
    "# Check the data types and any potential NaNs introduced during conversion\n",
    "print(\"Converted 'Systolic_BP' and 'Diastolic_BP' to numeric types:\")\n",
    "print(patient_data[['Systolic_BP', 'Diastolic_BP']].info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the Original 'Blood_Pressure' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Blood_Pressure' column as it is no longer needed\n",
    "patient_data = patient_data.drop(columns=['Blood_Pressure'])\n",
    "\n",
    "# Confirm the column is dropped\n",
    "print(\"'Blood_Pressure' column dropped:\")\n",
    "print(patient_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill Missing Values for Numerical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in numerical columns with the mean of each column\n",
    "patient_data.fillna(patient_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Check for remaining missing values in the DataFrame\n",
    "print(\"Filled missing values in numerical columns. Checking for nulls:\")\n",
    "print(patient_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill Missing Values for Categorical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in categorical columns with the mode (most frequent value) of each column\n",
    "for col in patient_data.select_dtypes(include=['object']).columns:\n",
    "    patient_data[col].fillna(patient_data[col].mode()[0], inplace=True)\n",
    "\n",
    "# Confirm no missing values remain in categorical columns\n",
    "print(\"Filled missing values in categorical columns. Checking for nulls:\")\n",
    "print(patient_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate Features and Target Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and target variable for model training\n",
    "X = patient_data.drop(columns=['Disease_Predictions', 'Patient_ID'])\n",
    "y = patient_data['Disease_Predictions']\n",
    "\n",
    "# Display the shapes to ensure correct separation\n",
    "print(\"Separated features (X) and target (y):\")\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode Target Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical target variable using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print unique classes to confirm encoding\n",
    "print(\"Encoded target variable:\")\n",
    "print(\"Classes:\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot Encode and Standardize Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features and standardize numerical features for better model performance\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Confirm the scaling and encoding by displaying shape and sample data\n",
    "print(\"Features encoded and standardized. Sample data:\")\n",
    "print(X_scaled[:5])\n",
    "print(X_scaled.shape)\n",
    "# Visualize the first few rows of the scaled and encoded features\n",
    "pd.DataFrame(X_scaled, columns=X.columns).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply SMOTE to Balance Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMOTE to balance the classes by oversampling the minority classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y_encoded)\n",
    "\n",
    "# Confirm the balanced classes\n",
    "print(\"Applied SMOTE to balance classes. New class distribution:\")\n",
    "print(np.bincount(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into Training and Test Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1: K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "knn_val_pred = knn.predict(X_val)\n",
    "\n",
    "# Evaluate KNN on the validation set\n",
    "print(\"KNN Validation Accuracy:\", accuracy_score(y_val, knn_val_pred))\n",
    "print(\"KNN Validation Classification Report:\\n\", classification_report(y_val, knn_val_pred))\n",
    "\n",
    "# Predict on the test set\n",
    "knn_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate KNN on the test set\n",
    "print(\"KNN Test Accuracy:\", accuracy_score(y_test, knn_test_pred))\n",
    "print(\"KNN Test Classification Report:\\n\", classification_report(y_test, knn_test_pred))\n",
    "\n",
    "# Plot confusion matrix for the test set\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test, knn_test_pred), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"KNN Model - Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found by GridSearchCV:\", best_params)\n",
    "\n",
    "# Predict on the validation set using the best KNN model\n",
    "knn_val_pred = best_knn.predict(X_val)\n",
    "\n",
    "# Evaluate the best KNN model on the validation set\n",
    "print(\"Best KNN Validation Accuracy:\", accuracy_score(y_val, knn_val_pred))\n",
    "print(\"Best KNN Validation Classification Report:\\n\", classification_report(y_val, knn_val_pred))\n",
    "\n",
    "# Predict on the test set using the best KNN model\n",
    "knn_test_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Evaluate the best KNN model on the test set\n",
    "print(\"Best KNN Test Accuracy:\", accuracy_score(y_test, knn_test_pred))\n",
    "print(\"Best KNN Test Classification Report:\\n\", classification_report(y_test, knn_test_pred))\n",
    "\n",
    "# Plot confusion matrix for the test set\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test, knn_test_pred), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Best KNN Model - Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = 'scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Save the label encoder\n",
    "label_encoder_filename = 'label_encoder.pkl'\n",
    "joblib.dump(label_encoder, label_encoder_filename)\n",
    "\n",
    "# Save the SMOTE object\n",
    "smote_filename = 'smote.pkl'\n",
    "joblib.dump(smote, smote_filename)\n",
    "\n",
    "# Save the KNN model\n",
    "knn_model_filename = 'knn_model.pkl'\n",
    "joblib.dump(best_knn, knn_model_filename)\n",
    "\n",
    "print(\"Preprocessing steps and KNN model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is low, so lets move to RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2: RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "rf_val_pred = rf.predict(X_val)\n",
    "\n",
    "# Evaluate RandomForest on the validation set\n",
    "print(\"RandomForest Validation Accuracy:\", accuracy_score(y_val, rf_val_pred))\n",
    "print(\"RandomForest Validation Classification Report:\\n\", classification_report(y_val, rf_val_pred))\n",
    "\n",
    "# Predict on the test set\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate RandomForest on the test set\n",
    "print(\"RandomForest Test Accuracy:\", accuracy_score(y_test, rf_test_pred))\n",
    "print(\"RandomForest Test Classification Report:\\n\", classification_report(y_test, rf_test_pred))\n",
    "\n",
    "# Plot confusion matrix for the test set\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test, rf_test_pred), annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\n",
    "plt.title(\"RandomForest Model - Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    \"\"\"\n",
    "    Plot feature importance for tree-based models\n",
    "    \"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f\"Feature Importance ({title})\")\n",
    "    plt.bar(range(len(importance)), importance[indices])\n",
    "    plt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot feature importance for Random Forest\n",
    "plot_feature_importance(rf, X.columns, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found by GridSearchCV for RandomForest:\", best_params_rf)\n",
    "\n",
    "# Predict on the validation set using the best RandomForest model\n",
    "rf_val_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Evaluate the best RandomForest model on the validation set\n",
    "print(\"Best RandomForest Validation Accuracy:\", accuracy_score(y_val, rf_val_pred))\n",
    "print(\"Best RandomForest Validation Classification Report:\\n\", classification_report(y_val, rf_val_pred))\n",
    "\n",
    "# Predict on the test set using the best RandomForest model\n",
    "rf_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the best RandomForest model on the test set\n",
    "print(\"Best RandomForest Test Accuracy:\", accuracy_score(y_test, rf_test_pred))\n",
    "print(\"Best RandomForest Test Classification Report:\\n\", classification_report(y_test, rf_test_pred))\n",
    "\n",
    "# Plot confusion matrix for the test set\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test, rf_test_pred), annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\n",
    "plt.title(\"Best RandomForest Model - Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RandomForest predictions as a new feature to the training and test sets\n",
    "X_train_with_rf = np.hstack((X_train, best_rf.predict(X_train).reshape(-1, 1)))\n",
    "X_val_with_rf = np.hstack((X_val, best_rf.predict(X_val).reshape(-1, 1)))\n",
    "X_test_with_rf = np.hstack((X_test, best_rf.predict(X_test).reshape(-1, 1)))\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_with_rf = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Fit the model to the training data with the additional RandomForest feature\n",
    "xgb_with_rf.fit(X_train_with_rf, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "xgb_with_rf_val_pred = xgb_with_rf.predict(X_val_with_rf)\n",
    "\n",
    "# Evaluate the XGBoost model with the additional RandomForest feature on the validation set\n",
    "print(\"XGBoost with RandomForest Feature Validation Accuracy:\", accuracy_score(y_val, xgb_with_rf_val_pred))\n",
    "print(\"XGBoost with RandomForest Feature Validation Classification Report:\\n\", classification_report(y_val, xgb_with_rf_val_pred))\n",
    "\n",
    "# Predict on the test set\n",
    "xgb_with_rf_test_pred = xgb_with_rf.predict(X_test_with_rf)\n",
    "\n",
    "# Evaluate the XGBoost model with the additional RandomForest feature on the test set\n",
    "print(\"XGBoost with RandomForest Feature Test Accuracy:\", accuracy_score(y_test, xgb_with_rf_test_pred))\n",
    "print(\"XGBoost with RandomForest Feature Test Classification Report:\\n\", classification_report(y_test, xgb_with_rf_test_pred))\n",
    "\n",
    "# Plot confusion matrix for the test set\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test, xgb_with_rf_test_pred), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"XGBoost with RandomForest Feature - Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is still low,so lets move to CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshape for CNN Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for CNN input, adding a third dimension for channels (required by Conv1D)\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=2)\n",
    "X_val_reshaped = np.expand_dims(X_val, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "# Confirm reshaping\n",
    "print(\"Data reshaped for CNN input:\")\n",
    "print(\"X_train reshaped shape:\", X_train_reshaped.shape)\n",
    "print(\"X_val reshaped shape:\", X_val_reshaped.shape)\n",
    "print(\"X_test reshaped shape:\", X_test_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Target Variable to Categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to categorical format for multi-class classification\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_val_categorical = to_categorical(y_val)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Confirm conversion\n",
    "print(\"Converted target to categorical format:\")\n",
    "print(\"y_train shape:\", y_train_categorical.shape)\n",
    "print(\"y_val shape:\", y_val_categorical.shape)\n",
    "print(\"y_test shape:\", y_test_categorical.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Deep Residual CNN Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a deep residual CNN model with Conv1D layers\n",
    "def build_residual_cnn(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # First Conv1D Block\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Second Conv1D Block with residual connection\n",
    "    residual = Conv1D(128, kernel_size=3, padding='same')(x)  # Adjust dimensions with padding\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, residual])  # Add residual connection\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # Third Conv1D Block\n",
    "    x = Conv1D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Flatten and Dense Layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_residual_cnn(input_shape=(X_train_reshaped.shape[1], 1), num_classes=y_train_categorical.shape[1])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "print(\"Deep residual CNN model defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer and categorical crossentropy loss for multi-class classification\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"Model compiled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Up Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for early stopping, model checkpoint, and reducing learning rate on plateau\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_residual_model.keras', monitor='val_loss', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Callbacks set up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the training data and validate on test data using callbacks\n",
    "history = model.fit(X_train_reshaped, y_train_categorical, epochs=100, batch_size=64, validation_data=(X_test_reshaped, y_test_categorical), callbacks=callbacks, verbose=1)\n",
    "print(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training history for the CNN model\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# After training the CNN\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Best Weights and Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights from training\n",
    "model.load_weights('best_residual_model.keras')\n",
    "print(\"Loaded best model weights.\")\n",
    "\n",
    "# Evaluate CNN using categorical predictions\n",
    "cnn_pred_prob = model.predict(X_test_reshaped)\n",
    "cnn_pred = np.argmax(cnn_pred_prob, axis=1)\n",
    "y_test_classes = np.argmax(y_test_categorical, axis=1)\n",
    "\n",
    "# Calculate and display metrics\n",
    "print(\"CNN Accuracy:\", accuracy_score(y_test_classes, cnn_pred))\n",
    "print(\"CNN Classification Report:\\n\", classification_report(y_test_classes, cnn_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "cm = confusion_matrix(y_test_classes, cnn_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Reds\", cbar=False)\n",
    "plt.title(\"CNN Model - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary & Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(models_results):\n",
    "    \"\"\"\n",
    "    Plot comparison of model performances\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    accuracies = []\n",
    "    names = []\n",
    "    \n",
    "    # KNN results\n",
    "    knn_val_accuracy = accuracy_score(y_val, knn_val_pred)\n",
    "    accuracies.append(knn_val_accuracy)\n",
    "    names.append('KNN')\n",
    "    \n",
    "    # Random Forest results  \n",
    "    rf_val_accuracy = accuracy_score(y_val, rf_val_pred)\n",
    "    accuracies.append(rf_val_accuracy)\n",
    "    names.append('Random Forest')\n",
    "\n",
    "    # XGBoost with RF results\n",
    "    xgb_rf_val_accuracy = accuracy_score(y_val, xgb_with_rf_val_pred)\n",
    "    accuracies.append(xgb_rf_val_accuracy)\n",
    "    names.append('XGBoost+RF')\n",
    "\n",
    "    # CNN results\n",
    "    cnn_val_accuracy = accuracy_score(np.argmax(y_val_categorical, axis=1), \n",
    "                                    np.argmax(model.predict(X_val_reshaped), axis=1))\n",
    "    accuracies.append(cnn_val_accuracy)\n",
    "    names.append('CNN')\n",
    "\n",
    "    # Plot accuracies\n",
    "    plt.bar(range(len(accuracies)), accuracies)\n",
    "    plt.xticks(range(len(accuracies)), names, rotation=45)\n",
    "    plt.title('Model Validation Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(accuracies):\n",
    "        plt.text(i, v + 0.01, f'{v:.3f}', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Store model results\n",
    "models_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics\n",
    "    \"\"\"\n",
    "    # Convert predictions to class labels if using CNN\n",
    "    if model_name == \"CNN\":\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix with improved visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'name': model_name,\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Evaluate models and store results\n",
    "models_results = [\n",
    "    evaluate_model(knn, y_test, knn_test_pred, \"KNN\"),\n",
    "    evaluate_model(best_rf, y_test, rf_test_pred, \"Random Forest\"),\n",
    "    evaluate_model(xgb_with_rf, y_test, xgb_with_rf_test_pred, \"XGBoost+RF\"),\n",
    "    evaluate_model(model, y_test_categorical, cnn_pred_prob, \"CNN\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for result in models_results:\n",
    "    print(f\"{result['name']}:\")\n",
    "    print(f\"  - Accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "best_model = max(models_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest performing model: {best_model['name']} with accuracy {best_model['accuracy']:.4f}\")\n",
    "\n",
    "# Add correlation matrix visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "numerical_cols = X.select_dtypes(include=['float64']).columns\n",
    "correlation_matrix = patient_data[numerical_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add learning curves for each model\n",
    "def plot_sklearn_learning_curve(estimator, title, X, y, ylim=None, cv=5,\n",
    "                              n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Plot learning curve for sklearn models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, \n",
    "        train_sizes=train_sizes,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_cnn_learning_curve(history):\n",
    "    \"\"\"\n",
    "    Plot learning curve for CNN model using training history\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('CNN Model Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves based on model type\n",
    "for result in models_results:\n",
    "    if result['name'] == 'CNN':\n",
    "        plot_cnn_learning_curve(history)\n",
    "    else:\n",
    "        plot_sklearn_learning_curve(\n",
    "            result['model'],\n",
    "            f\"Learning Curve ({result['name']})\",\n",
    "            X_train, y_train\n",
    "        )\n",
    "\n",
    "# Final conclusions\n",
    "print(\"\\nFinal Analysis and Conclusions:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"1. Best performing model: {best_model['name']}\")\n",
    "print(f\"2. Overall accuracy achieved: {best_model['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the XGBoost+RF model to a .pkl file\n",
    "joblib.dump(xgb_with_rf, 'xgboost_rf_model.pkl')\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Interface for the Application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipywidgets import widgets, Layout, HBox, VBox\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "# Load your dataset and set up encoders and scaler\n",
    "data_path = 'Patient_Health_Data.csv'\n",
    "patient_data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop the 'Patient_ID' column if it exists\n",
    "patient_data = patient_data.drop(columns=['Patient_ID'], errors='ignore')\n",
    "\n",
    "# Splitting 'Blood_Pressure' into 'Systolic_BP' and 'Diastolic_BP'\n",
    "patient_data[['Systolic_BP', 'Diastolic_BP']] = patient_data['Blood_Pressure'].str.split('/', expand=True)\n",
    "\n",
    "# Convert new columns to numeric, handle non-numeric entries by setting them to NaN\n",
    "patient_data['Systolic_BP'] = pd.to_numeric(patient_data['Systolic_BP'], errors='coerce')\n",
    "patient_data['Diastolic_BP'] = pd.to_numeric(patient_data['Diastolic_BP'], errors='coerce')\n",
    "\n",
    "# Drop the original 'Blood_Pressure' column as it's no longer needed\n",
    "patient_data = patient_data.drop(columns=['Blood_Pressure'])\n",
    "\n",
    "# Prepare label encoders and scaler based on training\n",
    "label_encoder = LabelEncoder()\n",
    "patient_data['Disease_Predictions'] = patient_data['Disease_Predictions'].fillna(patient_data['Disease_Predictions'].mode()[0])\n",
    "y_encoded = label_encoder.fit_transform(patient_data['Disease_Predictions'])\n",
    "\n",
    "# Fill missing values for numerical columns\n",
    "patient_data.fillna(patient_data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Fill missing values for categorical columns\n",
    "for col in patient_data.select_dtypes(include=['object']).columns:\n",
    "    patient_data[col] = patient_data[col].fillna(patient_data[col].mode()[0])\n",
    "\n",
    "# Define features and scaler based on training data\n",
    "numeric_features = patient_data.select_dtypes(include=[np.number]).columns.drop(['Disease_Predictions'], errors='ignore').tolist()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(patient_data[numeric_features])\n",
    "\n",
    "# Load pre-trained XGBoost+RF model\n",
    "xgb_with_rf = joblib.load('xgboost_rf_model.pkl')\n",
    "\n",
    "# Display title and introduction with enhanced styling\n",
    "display(HTML(\"<h1 style='color: #00796B; font-size: 30px; text-align: center;'>Patient Health Prediction System</h1>\"))\n",
    "display(HTML(\"<p style='color: #616161; font-size: 16px; text-align: center;'>Please provide the patient's health details below and click <strong style='color: #388E3C;'>Predict Disease</strong> for a health assessment.</p>\"))\n",
    "\n",
    "# Set up widgets with color themes, tooltips, and placeholders\n",
    "feature_widgets = {}\n",
    "\n",
    "# Create interactive sliders for numerical features with colorful handles and descriptions\n",
    "for feature in numeric_features:\n",
    "    tooltip = f\"Set the value for {feature.replace('_', ' ')}\"\n",
    "    if feature == 'Systolic_BP':\n",
    "        feature_widgets[feature] = widgets.IntSlider(value=120, min=40, max=200, step=1, description=\"Systolic BP\",\n",
    "                                                     style={'description_width': 'initial', 'handle_color': '#29B6F6'}, layout=Layout(width='100%'), tooltip=tooltip)\n",
    "    elif feature == 'Diastolic_BP':\n",
    "        feature_widgets[feature] = widgets.IntSlider(value=80, min=40, max=120, step=1, description=\"Diastolic BP\",\n",
    "                                                     style={'description_width': 'initial', 'handle_color': '#EF5350'}, layout=Layout(width='100%'), tooltip=tooltip)\n",
    "    elif feature == 'Temperature_C':\n",
    "        feature_widgets[feature] = widgets.FloatSlider(value=37.0, min=30.0, max=44.0, step=0.1, description=\"Temperature\",\n",
    "                                                       style={'description_width': 'initial', 'handle_color': '#66BB6A'}, layout=Layout(width='100%'), tooltip=tooltip)\n",
    "    else:\n",
    "        min_val = patient_data[feature].min()\n",
    "        max_val = patient_data[feature].max()\n",
    "        feature_widgets[feature] = widgets.IntSlider(value=int((min_val + max_val) / 2),\n",
    "                                                     min=int(min_val),\n",
    "                                                     max=int(max_val),\n",
    "                                                     step=1,\n",
    "                                                     description=feature.replace('_', ' ').title(),\n",
    "                                                     style={'description_width': 'initial', 'handle_color': '#FFA726'}, layout=Layout(width='100%'), tooltip=tooltip)\n",
    "\n",
    "# Dropdown menus for categorical features with placeholders and colors\n",
    "for feature in patient_data.select_dtypes(include=['object']).columns:\n",
    "    if feature != 'Disease_Predictions':\n",
    "        unique_values = patient_data[feature].dropna().unique()\n",
    "        options = [('Select...', None)] + [(str(val), str(val)) for val in sorted(unique_values)]\n",
    "        feature_widgets[feature] = widgets.Dropdown(options=options, description=feature.replace('_', ' ').title(),\n",
    "                                                    style={'description_width': 'initial'}, layout=Layout(width='100%'))\n",
    "\n",
    "# Organize the input widgets into two columns with stylish borders\n",
    "numeric_widgets = VBox([feature_widgets[feature] for feature in numeric_features], layout=Layout(width='48%', border='2px solid #B2DFDB', padding='20px', margin='10px'))\n",
    "categorical_widgets = VBox([feature_widgets[feature] for feature in patient_data.select_dtypes(include=['object']).columns if feature != 'Disease_Predictions'],\n",
    "                           layout=Layout(width='48%', border='2px solid #FFCDD2', padding='20px', margin='10px'))\n",
    "\n",
    "# Display organized input sections side by side\n",
    "display(HBox([numeric_widgets, categorical_widgets]))\n",
    "\n",
    "# Prediction button with enhanced styling and center alignment\n",
    "predict_button = widgets.Button(description=\"Predict Disease\", button_style='success', layout=Layout(width='200px', padding='10px', height='100%'), tooltip=\"Click to predict disease\")\n",
    "output = widgets.Output(layout=Layout(border='2px solid #00796B', padding='20px', margin='15px 0', background_color='#E0F2F1'))\n",
    "\n",
    "# Define the prediction function with output formatting\n",
    "def on_predict_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        \n",
    "        try:\n",
    "            # Load the model\n",
    "            xgb_rf_model = joblib.load('xgboost_rf_model.pkl')\n",
    "            \n",
    "            # Gather selected feature values\n",
    "            selected_features = {feature: widget.value for feature, widget in feature_widgets.items()}\n",
    "            input_data = pd.DataFrame([selected_features])\n",
    "            \n",
    "            # Fill missing values\n",
    "            for col in numeric_features:\n",
    "                if col in input_data.columns and input_data[col].isnull().any():\n",
    "                    input_data[col].fillna(patient_data[col].median(), inplace=True)\n",
    "            \n",
    "            for col in patient_data.select_dtypes(include=['object']).columns:\n",
    "                if col in input_data.columns and col != 'Disease_Predictions' and input_data[col].isnull().any():\n",
    "                    input_data[col].fillna(patient_data[col].mode()[0], inplace=True)\n",
    "\n",
    "            # Get reference data without one-hot encoding first\n",
    "            reference_data = patient_data.drop(columns=['Disease_Predictions'])\n",
    "            \n",
    "            # Create full one-hot encoding for categorical columns\n",
    "            categorical_cols = patient_data.select_dtypes(include=['object']).columns.drop('Disease_Predictions')\n",
    "            \n",
    "            # One-hot encode without dropping first\n",
    "            input_encoded = pd.get_dummies(input_data, columns=categorical_cols)\n",
    "            reference_encoded = pd.get_dummies(reference_data, columns=categorical_cols)\n",
    "            \n",
    "            # Add any missing columns from reference data\n",
    "            for col in reference_encoded.columns:\n",
    "                if col not in input_encoded.columns:\n",
    "                    input_encoded[col] = 0\n",
    "            \n",
    "            # Keep only columns that were in training data\n",
    "            input_encoded = input_encoded[reference_encoded.columns]\n",
    "            \n",
    "            # Scale numeric features\n",
    "            input_encoded[numeric_features] = scaler.transform(input_encoded[numeric_features])\n",
    "            \n",
    "            # Drop extra column to match 24 features\n",
    "            if input_encoded.shape[1] > xgb_rf_model.n_features_in_:\n",
    "                cols_to_drop = input_encoded.columns[-(input_encoded.shape[1] - xgb_rf_model.n_features_in_):]\n",
    "                input_encoded = input_encoded.drop(columns=cols_to_drop)\n",
    "            \n",
    "            # Verify final shape\n",
    "            #print(f\"Final input shape: {input_encoded.shape}\")\n",
    "            #print(f\"Expected features: {xgb_rf_model.n_features_in_}\")\n",
    "            \n",
    "            # Make prediction\n",
    "            predicted_class = xgb_rf_model.predict(input_encoded)[0]\n",
    "            disease = label_encoder.inverse_transform([predicted_class])\n",
    "            \n",
    "            # Display prediction\n",
    "            display(HTML(f\"<h3 style='color: #00796B;'>Prediction Result</h3>\"))\n",
    "            display(HTML(f\"<p style='font-size: 16px; color: #424242;'>Based on the input parameters, the predicted disease is: <strong style='color: #D32F2F;'>{disease[0]}</strong></p>\"))\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<p style='color: #D32F2F;'>Error making prediction: {str(e)}</p>\"))\n",
    "            print(f\"Debug info:\")\n",
    "            print(f\"Input shape: {input_encoded.shape}\")\n",
    "            print(f\"Expected features: {xgb_rf_model.n_features_in_}\")\n",
    "            print(f\"Available columns: {input_encoded.columns.tolist()}\")\n",
    "\n",
    "# Set up button interaction\n",
    "predict_button.on_click(on_predict_button_clicked)\n",
    "\n",
    "# Display the button in the center and output area below it\n",
    "display(HBox([predict_button], layout=Layout(justify_content='center')))\n",
    "display(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
